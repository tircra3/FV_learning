{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras.layers import GlobalAveragePooling3D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Concatenate, BatchNormalization, Activation, Conv3D,MaxPooling3D, Dense, Dropout, Flatten\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,CSVLogger\n",
    "from keras import regularizers\n",
    "from keras import losses\n",
    "from keras import backend as K\n",
    "from keras.callbacks import History\n",
    "from keras.layers.merge import Concatenate\n",
    "import keras\n",
    "import keras.layers as KL\n",
    "from keras.layers import Layer\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "## 混和精度 ##\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "#################################################################\n",
    "with open('ADNI_TRAIN_GMWMCSF_CN_1_600_race.pickle','rb') as f :\n",
    "    CN = pickle.load(f)\n",
    "with open('ADNI_TRAIN_GMWMCSF_MCI_1_600_race.pickle', 'rb') as f:\n",
    "    MCI = pickle.load(f)\n",
    "with open('ADNI_TRAIN_GMWMCSF_AD_1_600_race.pickle','rb') as f :\n",
    "    AD = pickle.load(f)\n",
    "#################################################################\n",
    "ff = pd.DataFrame(np.concatenate((CN,MCI,AD),axis = 0))\n",
    "\n",
    "train_npy_input_CN = np.load('ADNI_TRAIN_GMWMCSF_CN_1_600.npy')\n",
    "train_npy_input_MCI = np.load('ADNI_TRAIN_GMWMCSF_MCI_1_600.npy')\n",
    "TRAIN_npy_input = np.concatenate((train_npy_input_CN, train_npy_input_MCI),axis = 0)\n",
    "del train_npy_input_CN,train_npy_input_MCI\n",
    "train_npy_input_AD = np.load('ADNI_TRAIN_GMWMCSF_AD_1_600.npy')\n",
    "TRAIN_npy_input = np.concatenate((TRAIN_npy_input, train_npy_input_AD),axis = 0) \n",
    "del train_npy_input_AD\n",
    "\n",
    "ttt = np.zeros([1,11],dtype=\"float32\")\n",
    "fir = np.concatenate((ttt,ff),axis = 0)\n",
    "fir = fir[1:,:]\n",
    "\n",
    "TRAIN_pickle = [] \n",
    "for j in fir:\n",
    "    c = [j[4],j[5]] # age and PTGENDER\n",
    "    TRAIN_pickle.append(c)\n",
    "\n",
    "TRAIN_pickle = pd.DataFrame(TRAIN_pickle)\n",
    "TRAIN_pickle[1].replace(\"Female\", 0).replace(\"Male\", 1)\n",
    "\n",
    "TRAIN_pickle = np.asarray(TRAIN_pickle).astype('float32')\n",
    "#正規化\n",
    "TRAIN_pickle[:,0] = (TRAIN_pickle[:,0]-50)/10   #年齡正規化\n",
    "\n",
    "fir = pd.DataFrame(fir)\n",
    "fit_y = []\n",
    "for i in range(len(fir[7])):\n",
    "    if (fir[7][i] >= 0.0)&(fir[7][i] <= 0.25):\n",
    "        fit_y.append([1,0,0,0,0,0])\n",
    "    elif (fir[7][i] > 0.25)&(fir[7][i] <= 2.75):\n",
    "        fit_y.append([0,1,0,0,0,0])\n",
    "    elif (fir[7][i] > 2.75)&(fir[7][i] <= 4.25):\n",
    "        fit_y.append([0,0,1,0,0,0])\n",
    "    elif (fir[7][i] > 4.25)&(fir[7][i] <= 9.25):\n",
    "        fit_y.append([0,0,0,1,0,0])\n",
    "    elif (fir[7][i] > 9.25)&(fir[7][i] <= 15.75):\n",
    "        fit_y.append([0,0,0,0,1,0])\n",
    "    elif fir[7][i] > 15.75:\n",
    "        fit_y.append([0,0,0,0,0,1])\n",
    "##############  class_weights  #######################\n",
    "a = fit_y.count([1,0,0,0,0,0])\n",
    "b = fit_y.count([0,1,0,0,0,0])\n",
    "c = fit_y.count([0,0,1,0,0,0])\n",
    "d = fit_y.count([0,0,0,1,0,0])\n",
    "e = fit_y.count([0,0,0,0,1,0])\n",
    "f = fit_y.count([0,0,0,0,0,1])\n",
    "all_count = a+b+c+d+e+f\n",
    "sw_dic = [all_count/a ,all_count/b ,all_count/c ,all_count/d ,all_count/e ,all_count/f]\n",
    "del a,b,c,d,e,f\n",
    "######################################################\n",
    "######################################################\n",
    "######################################################\n",
    "sw = sw_dic[0]*np.ones(len(fit_y))\n",
    "for ii in range(len(fit_y)):\n",
    "    if fit_y[ii] == [1,0,0,0,0,0]:\n",
    "        sw[ii] = sw_dic[0]\n",
    "    elif fit_y[ii] == [0,1,0,0,0,0]:\n",
    "        sw[ii] = sw_dic[1]\n",
    "    elif fit_y[ii] == [0,0,1,0,0,0]:\n",
    "        sw[ii] = sw_dic[2]\n",
    "    elif fit_y[ii] == [0,0,0,1,0,0]:\n",
    "        sw[ii] = sw_dic[3]\n",
    "    elif fit_y[ii] == [0,0,0,0,1,0]:\n",
    "        sw[ii] = sw_dic[4]\n",
    "    elif fit_y[ii] == [0,0,0,0,0,1]:\n",
    "        sw[ii] = sw_dic[5]\n",
    "####################################################\n",
    "TRAIN_npy_input = np.array(TRAIN_npy_input)\n",
    "TRAIN_npy_input = np.concatenate((np.expand_dims(TRAIN_npy_input[:,:,:,:,0],axis=-1),np.expand_dims(TRAIN_npy_input[:,:,:,:,2],axis=-1)),axis=-1)\n",
    "TRAIN_npy_input =TRAIN_npy_input[:, 7:135, 12:108, :112, :]\n",
    "####################################################\n",
    "#shuffle dataset\n",
    "fit_y = np.asarray(fit_y).astype('float32')\n",
    "TRAIN_pickle_shuffle =np.array(TRAIN_pickle)\n",
    "indices = list(range(len(TRAIN_pickle_shuffle)))\n",
    "random.shuffle(indices)\n",
    "TRAIN_npy_input = TRAIN_npy_input[indices,:,:,:,:]\n",
    "TRAIN_pickle = TRAIN_pickle[indices,:]\n",
    "fit_y = fit_y[indices]\n",
    "sw = sw[indices]\n",
    "###################################################\n",
    "input_pickle = Input(shape=(TRAIN_pickle.shape[1],))\n",
    "input_img = Input(shape=(TRAIN_npy_input.shape[1], \n",
    "                         TRAIN_npy_input.shape[2],\n",
    "                         TRAIN_npy_input.shape[3],\n",
    "                         TRAIN_npy_input.shape[4])\n",
    "                  )\n",
    "\n",
    "######   val_set  ######\n",
    "with open('ADNI_TEST_set.pickle', 'rb') as f:\n",
    "    ff_val = pickle.load(f)          \n",
    "\n",
    "TEST_npy_input_val = np.load('ADNI_TEST_set.npy')\n",
    "\n",
    "ttt_val = np.zeros([1,11],dtype=\"float32\")\n",
    "fir_val = np.concatenate((ttt_val,ff_val),axis = 0)\n",
    "fir_val = fir_val[1:,:]\n",
    "\n",
    "TEST_pickle_val = []    \n",
    "for j_val in fir_val:\n",
    "    c_val = [j_val[4],j_val[5]] # age and PTGENDER\n",
    "    TEST_pickle_val.append(c_val)\n",
    "    \n",
    "TEST_pickle_val = pd.DataFrame(TEST_pickle_val)\n",
    "TEST_pickle_val[1].replace(\"Female\", 0).replace(\"Male\", 1)\n",
    "TEST_pickle_val = np.asarray(TEST_pickle_val).astype('float32')\n",
    "#正規化\n",
    "TEST_pickle_val[:,0] = (TEST_pickle_val[:,0]-50)/10   #年齡正規化\n",
    "\n",
    "fir_val = pd.DataFrame(fir_val)\n",
    "fit_y_val = []\n",
    "for i in range(len(fir_val[7])):\n",
    "    if (fir_val[7][i] >= 0.0)&(fir_val[7][i] <= 0.25):\n",
    "        fit_y_val.append([1,0,0,0,0,0])\n",
    "    elif (fir_val[7][i] > 0.25)&(fir_val[7][i] <= 2.75):\n",
    "        fit_y_val.append([0,1,0,0,0,0])\n",
    "    elif (fir_val[7][i] > 2.75)&(fir_val[7][i] <= 4.25):\n",
    "        fit_y_val.append([0,0,1,0,0,0])\n",
    "    elif (fir_val[7][i] > 4.25)&(fir_val[7][i] <= 9.25):\n",
    "        fit_y_val.append([0,0,0,1,0,0])\n",
    "    elif (fir_val[7][i] > 9.25)&(fir_val[7][i] <= 15.75):\n",
    "        fit_y_val.append([0,0,0,0,1,0])\n",
    "    elif fir_val[7][i] > 15.75:\n",
    "        fit_y_val.append([0,0,0,0,0,1])\n",
    "        \n",
    "##############  class_weights  #######################\n",
    "g = fit_y_val.count([1,0,0,0,0,0])\n",
    "h = fit_y_val.count([0,1,0,0,0,0])\n",
    "i = fit_y_val.count([0,0,1,0,0,0])\n",
    "j = fit_y_val.count([0,0,0,1,0,0])\n",
    "k = fit_y_val.count([0,0,0,0,1,0])\n",
    "l = fit_y_val.count([0,0,0,0,0,1])\n",
    "all_count_val = g+h+i+j+k+l\n",
    "sw_dic_val = [all_count_val/g ,all_count_val/h ,all_count_val/i ,\n",
    "           all_count_val/j ,all_count_val/k ,all_count_val/l]\n",
    "del g ,h,i,j,k,l,all_count_val\n",
    "\n",
    "sw_val = sw_dic_val[0]*np.ones(len(fit_y_val))\n",
    "for ii in range(len(fit_y_val)):\n",
    "    if fit_y_val[ii] == [1,0,0,0,0,0]:\n",
    "        sw_val[ii] = sw_dic_val[0]\n",
    "    elif fit_y_val[ii] == [0,1,0,0,0,0]:\n",
    "        sw_val[ii] = sw_dic_val[1]\n",
    "    elif fit_y_val[ii] == [0,0,1,0,0,0]:\n",
    "        sw_val[ii] = sw_dic_val[2]\n",
    "    elif fit_y_val[ii] == [0,0,0,1,0,0]:\n",
    "        sw_val[ii] = sw_dic_val[3]\n",
    "    elif fit_y_val[ii] == [0,0,0,0,1,0]:\n",
    "        sw_val[ii] = sw_dic_val[4]\n",
    "    elif fit_y_val[ii] == [0,0,0,0,0,1]:\n",
    "        sw_val[ii] = sw_dic_val[5]\n",
    "\n",
    "########################################################\n",
    "TEST_npy_input_val = np.array(TEST_npy_input_val)\n",
    "TEST_npy_input_val = np.concatenate((np.expand_dims(TEST_npy_input_val[:,:,:,:,0],axis=-1),np.expand_dims(TEST_npy_input_val[:,:,:,:,2],axis=-1)),axis=-1)\n",
    "TEST_npy_input_val =TEST_npy_input_val[:, 7:135, 12:108, :112, :]\n",
    "#########################################################\n",
    "#shuffle dataset\n",
    "fit_y_val = np.asarray(fit_y_val).astype('float32')\n",
    "TEST_pickle_shuffle_val =np.array(TEST_pickle_val)\n",
    "indices_val = list(range(len(TEST_pickle_shuffle_val)))\n",
    "random.shuffle(indices_val)\n",
    "TEST_npy_input_val = TEST_npy_input_val[indices_val,:,:,:,:]\n",
    "TEST_pickle_val = TEST_pickle_val[indices_val,:]\n",
    "fit_y_val = fit_y_val[indices_val,:]\n",
    "sw_val = sw_val[indices_val]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "def dice_coef(y_true, y_pred, smooth=1e-5):\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true * y_true) + K.sum(y_pred * y_pred) + smooth)\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -K.log(dice_coef(y_true, y_pred))\n",
    "######################    CBAM_block   #################\n",
    "channel_axis = 1 if K.image_data_format() == \"channel_first\" else 4\n",
    "def channel_attention(input_xs, reduction_ratio=0.125):\n",
    "    # get channel\n",
    "    channel = int(input_xs.shape[channel_axis])\n",
    "    maxpool_channel = KL.GlobalMaxPooling3D()(input_xs)\n",
    "    maxpool_channel = KL.Reshape((1,1,1,channel))(maxpool_channel)\n",
    "    avgpool_channel = KL.GlobalAveragePooling3D()(input_xs)\n",
    "    avgpool_channel = KL.Reshape((1,1,1,channel))(avgpool_channel) \n",
    "    Dense_One = KL.Dense(units=int(channel*reduction_ratio), activation='selu', kernel_initializer='he_normal', use_bias=True, bias_initializer='zeros')\n",
    "    Dense_Two = KL.Dense(units=int(channel), activation='selu', kernel_initializer='he_normal', use_bias=True, bias_initializer='zeros')\n",
    "    # max path\n",
    "    mlp_1_max = Dense_One(maxpool_channel)\n",
    "    mlp_2_max = Dense_Two(mlp_1_max)\n",
    "    mlp_2_max = KL.Reshape(target_shape=(1,1,1,int(channel)))(mlp_2_max)\n",
    "    # avg path\n",
    "    mlp_1_avg = Dense_One(avgpool_channel)\n",
    "    mlp_2_avg = Dense_Two(mlp_1_avg)\n",
    "    mlp_2_avg = KL.Reshape(target_shape=(1,1,1,int(channel)))(mlp_2_avg)\n",
    "    \n",
    "    channel_attention_feature = KL.Add()([mlp_2_max, mlp_2_avg])\n",
    "    channel_attention_feature = KL.Activation('sigmoid')(channel_attention_feature)\n",
    "    return KL.Multiply()([channel_attention_feature, input_xs])\n",
    "\n",
    "def spatial_attention(channel_refined_feature):\n",
    "    maxpool_spatial = KL.Lambda(lambda x: K.max(x, axis=4, keepdims=True))(channel_refined_feature)\n",
    "    avgpool_spatial = KL.Lambda(lambda x: K.mean(x, axis=4, keepdims=True))(channel_refined_feature)\n",
    "    max_avg_pool_spatial = KL.Concatenate(axis=4)([maxpool_spatial, avgpool_spatial])\n",
    "    return KL.Conv3D(filters=1, kernel_size=(3,3,3), padding=\"same\", activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(max_avg_pool_spatial)\n",
    "\n",
    "def cbam_module(input_xs, reduction_ratio=0.5):\n",
    "    channel_refined_feature = channel_attention(input_xs, reduction_ratio=reduction_ratio)\n",
    "    spatial_attention_feature = spatial_attention(channel_refined_feature)\n",
    "    refined_feature = KL.Multiply()([channel_refined_feature,spatial_attention_feature])\n",
    "    return KL.Add()([refined_feature, input_xs])\n",
    "#########################################################\n",
    "ipt1 = input_img\n",
    "\n",
    "mod1_a = Conv3D(32, kernel_size=(3,3,3), padding='same', kernel_initializer='glorot_normal')(ipt1)\n",
    "mod1_1 = BatchNormalization()(mod1_a)\n",
    "mod1_1 = Concatenate(axis = -1)([mod1_1,ipt1])   #res\n",
    "mod1_1 = Activation(\"selu\")(mod1_1)\n",
    "mod1_1 = cbam_module(mod1_1, reduction_ratio=0.125)\n",
    "mod1_1 = MaxPooling3D(pool_size=(2,2,2), padding='same')(mod1_1)\n",
    "del ipt1\n",
    "mod1_2 = Conv3D(32, kernel_size=(3,3,3), padding='same',kernel_initializer='glorot_normal')(mod1_1)\n",
    "mod1_2 = BatchNormalization()(mod1_2)\n",
    "mod1_2 = Concatenate(axis = -1)([mod1_2,mod1_1])   #res\n",
    "mod1_2 = Activation(\"selu\")(mod1_2)\n",
    "mod1_2 = MaxPooling3D(pool_size=(2,2,2), padding='same')(mod1_2)\n",
    "\n",
    "mod1_3 = Conv3D(32, kernel_size=(3,3,3), padding='same',kernel_initializer='glorot_normal')(mod1_2)\n",
    "mod1_3 = BatchNormalization()(mod1_3)\n",
    "mod1_3 = Concatenate(axis = -1)([mod1_3,mod1_2])   #res\n",
    "mod1_3 = Activation(\"selu\")(mod1_3)\n",
    "mod1_3 = cbam_module(mod1_3, reduction_ratio=0.125)\n",
    "mod1_3 = MaxPooling3D(pool_size=(2,2,2), padding='same')(mod1_3)\n",
    "\n",
    "mod1_4 = Conv3D(32, kernel_size=(3,3,3), padding='same',kernel_initializer='glorot_normal')(mod1_3)\n",
    "mod1_4 = BatchNormalization()(mod1_4)\n",
    "mod1_4 = Concatenate(axis = -1)([mod1_4,mod1_3])   #res\n",
    "mod1_4 = Activation(\"selu\")(mod1_4)\n",
    "mod1_4 = MaxPooling3D(pool_size=(2,2,2), padding='same')(mod1_4)\n",
    "\n",
    "mod1_5 = Conv3D(32, kernel_size=(3,3,3), padding='same',kernel_initializer='glorot_normal')(mod1_4)\n",
    "mod1_5 = BatchNormalization()(mod1_5)\n",
    "mod1_5 = Concatenate(axis = -1)([mod1_5,mod1_4])   #res\n",
    "mod1_5 = Activation(\"selu\")(mod1_5)\n",
    "mod1_5 = cbam_module(mod1_5, reduction_ratio=0.125)\n",
    "mod1_5 = MaxPooling3D(pool_size=(2,2,2), padding='same')(mod1_5)\n",
    "mod1_5 = Dropout(0.4)(mod1_5)\n",
    "opt = Conv3D(32,kernel_size=(1,1,1), padding='same',kernel_initializer='glorot_normal')(mod1_5)\n",
    "opt = BatchNormalization()(opt)\n",
    "opt = Activation(\"selu\")(opt)\n",
    "opt = Flatten()(opt)\n",
    "opt = Dropout(0.4)(opt)\n",
    "out_1 = Dense(32,activation=None,kernel_initializer='glorot_normal',kernel_regularizer= regularizers.l2(0.03))(opt)\n",
    "out_1 = Activation('tanh')(out_1)\n",
    "out_1 = Concatenate(axis=1)([out_1, input_pickle])\n",
    "out_1 = Dense(32,activation=None,kernel_initializer='glorot_normal',kernel_regularizer= regularizers.l2(0.03))(out_1)\n",
    "out_1 = Activation('tanh')(out_1)\n",
    "out_1 = Dense(32,activation=None,kernel_initializer='glorot_normal',kernel_regularizer= regularizers.l2(0.03))(out_1)\n",
    "out_1 = Activation('tanh')(out_1)\n",
    "out_1 = Dense(32,activation=None,kernel_initializer='glorot_normal',kernel_regularizer= regularizers.l2(0.03))(out_1)\n",
    "out_1 = Activation('tanh')(out_1)\n",
    "out_1 = Dense(6,activation='softmax',kernel_initializer='glorot_normal',dtype='float32', kernel_regularizer= regularizers.l2(0.03))(out_1)\n",
    "\n",
    "################  load_weights ################\n",
    "model = Model(inputs =[input_img, input_pickle], outputs = out_1)\n",
    "################################################\n",
    "metrics = ['accuracy',\n",
    "           dice_coef]\n",
    "                                     \n",
    "optimizers = keras.optimizers.Adam(lr = 5e-4)\n",
    "model.compile(optimizer=optimizers, loss = dice_coef_loss , metrics = metrics) \n",
    "\n",
    "model.summary()\n",
    "\n",
    "## callback\n",
    "history = History()\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-20, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint('6CDRclass_cut_conv5_32_4d32_D04_flatten_D04_mix_p5_cbam135-0125_dice_lr-4_l2003_b12-{epoch:05d}-{val_accuracy:.5f}' + '.h5', monitor='val_accuracy', verbose=1, save_best_only=False, mode='max')\n",
    "csv_logger = CSVLogger(filename = '6CDRclass_cut_conv5_32_4d32_D04_flatten_D04_mix_p5_cbam135-0125_dice_lr-4_l2003_b12.csv' ,separator=',' ,append=False)\n",
    "\n",
    "model.fit([TRAIN_npy_input, TRAIN_pickle],fit_y, sample_weight = sw, batch_size = 12 ,epochs = 200,verbose = 1,callbacks = [history,reduce_lr , model_checkpoint,csv_logger], validation_data =([TEST_npy_input_val, TEST_pickle_val],fit_y_val,sw_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c55525a091aabe8b70a90459366577a5b3136df3bf103cf104694379bfad88aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
